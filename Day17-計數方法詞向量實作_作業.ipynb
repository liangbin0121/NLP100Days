{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"cupoy_env","language":"python","name":"cupoy_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Day17-計數方法詞向量實作_作業.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"hViNRYK17-QO"},"source":["### 載入所需的Libraries"]},{"cell_type":"code","metadata":{"id":"lURY7TXh8GSS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610326652033,"user_tz":-480,"elapsed":24992,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"edf7bce5-7780-4ddb-f74b-7a08495db178"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRw_xGC98GVx","executionInfo":{"status":"ok","timestamp":1610326658047,"user_tz":-480,"elapsed":1016,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"b37d10d9-c046-4397-c5c4-35cb23c5d73f"},"source":["import os\r\n","\r\n","# Current directory\r\n","print(os.getcwd())\r\n","\r\n","# change directory\r\n","os.chdir('/content/drive/MyDrive/python_training/NLP100Days/day_17-word_embedding-count/')\r\n","print(os.getcwd())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/MyDrive/python_training/NLP100Days/day_17-word_embedding-count\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gRGVIowa7-QU","executionInfo":{"status":"ok","timestamp":1610326661585,"user_tz":-480,"elapsed":1648,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}}},"source":["import re\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from typing import List"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlXkazak7-QV"},"source":["### 載入資料"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":509},"id":"gDtmjN4K7-QV","executionInfo":{"status":"error","timestamp":1610326685814,"user_tz":-480,"elapsed":897,"user":{"displayName":"蔡良彬","photoUrl":"","userId":"04145839673132622291"}},"outputId":"b53ad3bb-5d5f-4819-c216-a154e1467818"},"source":["# read data from spam.csv\n","sms_data=pd.read_csv('spam.csv',encoding='utf-8') #',encoding='utf-8',ANSI\n","###<your code>###\n","sms_data.head()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"UnicodeDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6161875b1da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read data from spam.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msms_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spam.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#',encoding='utf-8',ANSI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m###<your code>###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msms_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte"]}]},{"cell_type":"markdown","metadata":{"id":"jyz_bmwc9PkB"},"source":["\r\n","label\tcontent\r\n","0\tham\tGo until jurong point, crazy.. Available only ...\r\n","\r\n","1\tham\tOk lar... Joking wif u oni...\r\n","\r\n","2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\r\n","\r\n","3\tham\tU dun say so early hor... U c already then say...\r\n","\r\n","4\tham\tNah I don't think he goes to usf, he lives aro..."]},{"cell_type":"code","metadata":{"id":"W4KOCVhU7-QW"},"source":["# check how many spams and hams\n","###<your code>###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kc4rB2QU7-QX"},"source":["# change label to from string to number\n","# \"ham\" --> 0, \"spam\" --> 1\n","\n","###<your code>###\n","sms_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJyeLhOz7-QX"},"source":["### 切分資料\n","將資料依據label比例切分為training data與testing data"]},{"cell_type":"code","metadata":{"id":"zvnNigoA7-QX"},"source":["###<your code>###\n","\n","print(sms_train.groupby(\"label\").count())\n","print(sms_test.groupby(\"label\").count())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oByGjhOM7-QX"},"source":["### 資料預處理\n","* 將所有字詞轉為小寫\n","* 移除所有數字、標點符號"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tONFhyrD7-QY"},"source":["def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n","    \n","    for i in range(len(df)):\n","        # make all content to lowercase\n","        ###<your code>###\n","\n","        # remove all punctuations\n","        ###<your code>###\n","    \n","    return df\n","\n","processed_train = preprocess(sms_train)\n","processed_test = preprocess(sms_test)\n","\n","print(\"Train:\")\n","print(processed_train.head())\n","print(\"Test:\")\n","print(processed_test.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKvddf657-QY"},"source":["# split data into x_train, y_train, x_test, y_test\n","y_train, x_train = zip(*processed_train.values)\n","y_test, x_test = zip(*processed_test.values)\n","\n","# check numbers of unique word in the corpus\n","len(set(\" \".join(list(x_train + x_test)).split()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAYKXyMu7-QY"},"source":["### TF-IDF\n","可以發現種共有7708個字詞，這裡使用TF-IDF將來選取最高分的前2000個字詞\n","(若忘記的學員可參考先前TF-IDF課程章節或[此篇教學](https://ithelp.ithome.com.tw/articles/10228815?sc=iThelpR))"]},{"cell_type":"code","metadata":{"id":"DC9eDTPu7-QZ"},"source":["###<your code>###\n","\n","print(f\"Number of unique word: {len(vectorizer.vocabulary_)}\")\n","(tfidf_train, tfidf_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwkjabB37-QZ"},"source":["### 建立共現矩陣"]},{"cell_type":"code","metadata":{"id":"k-mIWpQo7-QZ"},"source":["def create_co_matrix(corpus: List[str], vocab_list: List[str], word2idx: dict,\n","                     window_size: int=1, use_weighting: bool=False, verbose: bool=False) -> np.ndarray:\n","    '''Function to create co-occurrence matrix\n","    '''\n","    #initialize co-occurrence matrix\n","    ###<your code>###\n","    \n","    for idx, sms in enumerate(corpus):\n","        ###<your code>###\n","        \n","        for center_i, center_word_id in enumerate(sms_ids):\n","            ###<your code>###\n","            \n","            for left_i, left_word_id in enumerate(context_ids):\n","                \n","                ###<your code>###\n","        \n","        if verbose:\n","            if idx != 0 and idx%500 == 0:\n","                    print(f\"finishing {idx+1}/{len(corpus)}\")\n","    print(\"Done\")\n","    if use_weighting:\n","        # if use weighting, then we set the co-occurrence with the word itself to 1.0\n","        ###<your code>###\n","        \n","    return co_matrix\n","\n","co_matrix = create_co_matrix(x_train, vectorizer.get_feature_names(), vectorizer.vocabulary_,\n","                            window_size=3, use_weighting=True, verbose=True)\n","\n","co_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2WqFgRJs7-QZ"},"source":["### 建立PPMI矩陣"]},{"cell_type":"code","metadata":{"id":"nJlSDiFw7-Qa"},"source":["#定義正向點間互資訊\n","\n","def ppmi(co_matrix: np.ndarray, eps: float=1e-8, verbose: bool=False):\n","    ###<your code>###\n","    \n","    for i in range(co_matrix.shape[0]):\n","        for j in range(co_matrix.shape[1]):\n","            ###<your code>###\n","            \n","            if verbose:\n","                cnt += 1\n","                if cnt % 10 == 0 or cnt == total:\n","                    print(f\"{cnt}/{total} Done\")\n","    \n","    return M\n","\n","ppmi_matrix = ppmi(co_matrix, verbose=False)\n","ppmi_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPXJVcb47-Qa"},"source":["### 使用SVD降維\n","利用sklearn中的TruncatedSVD對co-occurrence matrix進行降維，並利用variance來找出最適合的維度\n","[參考文獻](https://medium.com/swlh/truncated-singular-value-decomposition-svd-using-amazon-food-reviews-891d97af5d8d)\n","\n","(讀者可以嘗試使用SVD對PPMI進行降維)"]},{"cell_type":"code","metadata":{"id":"qemxqdy97-Qa"},"source":["# Program to find the optimal number of components for Truncated SVD\n","n_comp = range(10,150,10) # list containing different values of components\n","variance_sum = [] # explained variance ratio for each component of Truncated SVD\n","\n","for dim in n_comp:\n","    ###<your code>###\n","    \n","plt.plot(n_comp, variance_sum)\n","plt.xlabel('Number of components')\n","plt.ylabel(\"Explained Variance\")\n","plt.title(\"Plot of Number of components v/s explained variance\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YF7uBx_17-Qa"},"source":["# choose 140 as final dimension to reduce to \n","# 利用上述找到的最適合dimension來對co-occurrence matrix進行降維\n","###<your code>###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAD1pnzE7-Qa"},"source":["### 使用KNN模型進行分類\n","在進行分類之前，先利用簡單的詞向量平均來計算文本向量\n","\n","[參考文獻](https://medium.com/ai-academy-taiwan/nlp-%E4%B8%8D%E5%90%8C%E8%A9%9E%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E5%88%86%E9%A1%9E%E4%B8%8A%E7%9A%84%E8%A1%A8%E7%8F%BE%E8%88%87%E5%AF%A6%E4%BD%9C-e72a2daecfc)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"M3QA65hQ7-Qb"},"source":["# get doc vector via take mean of all word vectors inside the corresponding document\n","\n","def make_doc_vectors(corpus: List[str], word2idx: dict, vocab_list: List) -> List[np.ndarray]:\n","    \n","    # vectorizing data \n","    # and make document vector by take mean to all word vecto\n","    doc_vec = []\n","    empty_doc_list = []\n","    for i, sms_msg in enumerate(corpus):\n","        sms_msg = [word2idx[word] for word in sms_msg.split() if word in vocab_list] #tokenize\n","        if len(sms_msg) > 0:\n","            sms_msg = np.array([re_co_matrix[ids] for ids in sms_msg]) #vectorize\n","            doc_vec.append(sms_msg.mean(axis=0))\n","        else:\n","            empty_doc_list.append(i)\n","            print(f\"document {i} doesn't contain word in vocab_list\")\n","            print(corpus[i])\n","            print(\"\\n\")\n","        \n","    return np.vstack(doc_vec), empty_doc_list\n","\n","word2idx = vectorizer.vocabulary_\n","vocab_list = vectorizer.get_feature_names()\n","\n","doc_vec_train, missing_train_list = make_doc_vectors(x_train, word2idx, vocab_list)\n","print(\"=\"*50)\n","doc_vec_test, missing_test_list = make_doc_vectors(x_test, word2idx, vocab_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Deh35OPr7-Qb"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# training\n","y_train_filter = np.delete(np.array(y_train), missing_train_list)\n","\n","###<your code>###\n","\n","# testing\n","y_test_filter = np.delete(np.array(y_test), missing_test_list)\n","###<your code>###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l-X-0Og7-Qb"},"source":["print(f\"train acc: {np.sum(train_pred == y_train_filter) / len(y_train_filter)}\")\n","print(f\"train acc: {np.sum(test_pred == y_test_filter) / len(y_test_filter)}\")"],"execution_count":null,"outputs":[]}]}